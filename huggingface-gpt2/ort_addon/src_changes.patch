diff --git a/examples/run_language_modeling.py b/examples/run_language_modeling.py
index 660a652..c18319d 100644
--- a/examples/run_language_modeling.py
+++ b/examples/run_language_modeling.py
@@ -19,7 +19,6 @@ GPT and GPT-2 are fine-tuned using a causal language modeling (CLM) loss while B
 using a masked language modeling (MLM) loss.
 """
 
-
 import logging
 import math
 import os
@@ -38,6 +37,7 @@ from transformers import (
     PreTrainedTokenizer,
     TextDataset,
     Trainer,
+    OrtTrainer,
     TrainingArguments,
     set_seed,
 )
@@ -45,6 +45,9 @@ from transformers import (
 
 logger = logging.getLogger(__name__)
 
+from azureml.core.run import Run
+# get the Azure ML run object
+run = Run.get_context()
 
 MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())
 MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)
@@ -135,6 +138,8 @@ def main():
     parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))
     model_args, data_args, training_args = parser.parse_args_into_dataclasses()
 
+    training_args.update_args()
+
     if data_args.eval_data_file is None and training_args.do_eval:
         raise ValueError(
             "Cannot do evaluation without an evaluation data file. Either supply a file to --eval_data_file "
@@ -234,8 +239,9 @@ def main():
         tokenizer=tokenizer, mlm=data_args.mlm, mlm_probability=data_args.mlm_probability
     )
 
+    trainer = OrtTrainer if training_args.ort_trainer else Trainer
     # Initialize our Trainer
-    trainer = Trainer(
+    trainer = trainer(
         model=model,
         args=training_args,
         data_collator=data_collator,
@@ -274,6 +280,7 @@ def main():
             for key in sorted(result.keys()):
                 logger.info("  %s = %s", key, str(result[key]))
                 writer.write("%s = %s\n" % (key, str(result[key])))
+                run.log(key,result[key])
 
         results.update(result)
 
diff --git a/src/transformers/__init__.py b/src/transformers/__init__.py
index 19c4dc7..50fe461 100755
--- a/src/transformers/__init__.py
+++ b/src/transformers/__init__.py
@@ -326,6 +326,7 @@ if is_torch_available():
 
     # Trainer
     from .trainer import Trainer, set_seed, torch_distributed_zero_first, EvalPrediction
+    from .trainer_ort import OrtTrainer
     from .data.data_collator import DefaultDataCollator, DataCollator, DataCollatorForLanguageModeling
     from .data.datasets import GlueDataset, TextDataset, LineByLineTextDataset, GlueDataTrainingArguments
 
diff --git a/src/transformers/activations.py b/src/transformers/activations.py
index 2028e9c..e45606a 100644
--- a/src/transformers/activations.py
+++ b/src/transformers/activations.py
@@ -26,7 +26,7 @@ def gelu_new(x):
     """ Implementation of the gelu activation function currently in Google Bert repo (identical to OpenAI GPT).
         Also see https://arxiv.org/abs/1606.08415
     """
-    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))
+    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))
 
 
 if torch.__version__ < "1.4.0":
@@ -41,7 +41,8 @@ else:
             " no activation function will be traced with JIT."
         )
     except ImportError:
-        gelu_new = torch.jit.script(gelu_new)
+        # gelu_new = torch.jit.script(gelu_new)
+        pass
 
 ACT2FN = {
     "relu": F.relu,
diff --git a/src/transformers/modeling_gpt2.py b/src/transformers/modeling_gpt2.py
index 1201399..87c8e1e 100644
--- a/src/transformers/modeling_gpt2.py
+++ b/src/transformers/modeling_gpt2.py
@@ -112,8 +112,8 @@ class Attention(nn.Module):
         self.split_size = n_state
         self.scale = scale
 
-        self.c_attn = Conv1D(n_state * 3, nx)
-        self.c_proj = Conv1D(n_state, nx)
+        self.c_attn = Conv1D(n_state * 3, nx, n_ctx)
+        self.c_proj = Conv1D(n_state, nx, n_ctx)
         self.attn_dropout = nn.Dropout(config.attn_pdrop)
         self.resid_dropout = nn.Dropout(config.resid_pdrop)
         self.pruned_heads = set()
@@ -145,7 +145,7 @@ class Attention(nn.Module):
         if self.scale:
             w = w / math.sqrt(v.size(-1))
         nd, ns = w.size(-2), w.size(-1)
-        mask = self.bias[:, :, ns - nd : ns, :ns]
+        mask = self.bias.to(torch.bool)
         w = torch.where(mask, w, self.masked_bias)
 
         if attention_mask is not None:
@@ -166,11 +166,11 @@ class Attention(nn.Module):
 
     def merge_heads(self, x):
         x = x.permute(0, 2, 1, 3).contiguous()
-        new_x_shape = x.size()[:-2] + (x.size(-2) * x.size(-1),)
-        return x.view(*new_x_shape)  # in Tensorflow implem: fct merge_states
+        new_x_shape = x.size()[:-2]
+        return x.view(*new_x_shape, -1)  # in Tensorflow implem: fct merge_states
 
     def split_heads(self, x, k=False):
-        new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)
+        new_x_shape = x.size()[:-1] + (self.n_head, self.split_size // self.n_head)
         x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states
         if k:
             return x.permute(0, 2, 3, 1)  # (batch, head, head_features, seq_length)
@@ -179,7 +179,7 @@ class Attention(nn.Module):
 
     def forward(self, x, layer_past=None, attention_mask=None, head_mask=None, use_cache=False):
         x = self.c_attn(x)
-        query, key, value = x.split(self.split_size, dim=2)
+        query, key, value = torch.split(x, [self.split_size,self.split_size,self.split_size], dim=2)
         query = self.split_heads(query)
         key = self.split_heads(key, k=True)
         value = self.split_heads(value)
@@ -208,8 +208,8 @@ class MLP(nn.Module):
     def __init__(self, n_state, config):  # in MLP: n_state=3072 (4 * n_embd)
         super().__init__()
         nx = config.n_embd
-        self.c_fc = Conv1D(n_state, nx)
-        self.c_proj = Conv1D(nx, n_state)
+        self.c_fc = Conv1D(n_state, nx, config.n_ctx)
+        self.c_proj = Conv1D(nx, n_state, config.n_ctx)
         self.act = ACT2FN[config.activation_function]
         self.dropout = nn.Dropout(config.resid_pdrop)
 
@@ -349,6 +349,7 @@ class GPT2Model(GPT2PreTrainedModel):
         self.drop = nn.Dropout(config.embd_pdrop)
         self.h = nn.ModuleList([Block(config.n_ctx, config, scale=True) for _ in range(config.n_layer)])
         self.ln_f = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)
+        self.seq_length = config.n_ctx
 
         self.init_weights()
 
@@ -425,7 +426,7 @@ class GPT2Model(GPT2PreTrainedModel):
             raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
         elif input_ids is not None:
             input_shape = input_ids.size()
-            input_ids = input_ids.view(-1, input_shape[-1])
+            # input_ids = input_ids.view(-1, input_shape[-1])
             batch_size = input_ids.shape[0]
         elif inputs_embeds is not None:
             input_shape = inputs_embeds.size()[:-1]
@@ -445,8 +446,8 @@ class GPT2Model(GPT2PreTrainedModel):
             past_length = past[0][0].size(-2)
         if position_ids is None:
             device = input_ids.device if input_ids is not None else inputs_embeds.device
-            position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.long, device=device)
-            position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])
+            position_ids = torch.arange(past_length, self.seq_length + past_length, dtype=torch.long, device=device)
+            position_ids = position_ids.unsqueeze(0).view(-1, self.seq_length)
 
         # Attention mask.
         if attention_mask is not None:
@@ -626,11 +627,11 @@ class GPT2LMHeadModel(GPT2PreTrainedModel):
             shift_labels = labels[..., 1:].contiguous()
             # Flatten the tokens
             loss_fct = CrossEntropyLoss()
-            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
+            loss = loss_fct(shift_logits.view(-1, self.config.vocab_size), shift_labels.view(-1))
             outputs = (loss,) + outputs
 
         return outputs  # (loss), lm_logits, presents, (all hidden_states), (attentions)
-
+        # return loss
 
 @add_start_docstrings(
     """The GPT2 Model transformer with a language modeling and a multiple-choice classification
diff --git a/src/transformers/modeling_utils.py b/src/transformers/modeling_utils.py
index 1dc7a7d..11bf886 100644
--- a/src/transformers/modeling_utils.py
+++ b/src/transformers/modeling_utils.py
@@ -1692,20 +1692,22 @@ class BeamHypotheses(object):
 
 
 class Conv1D(nn.Module):
-    def __init__(self, nf, nx):
+    def __init__(self, nf, nx, n_ctx):
         """ Conv1D layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2)
             Basically works like a Linear layer but the weights are transposed
         """
         super().__init__()
         self.nf = nf
+        self.nx = nx
+        self.n_ctx = n_ctx
         w = torch.empty(nx, nf)
         nn.init.normal_(w, std=0.02)
         self.weight = nn.Parameter(w)
         self.bias = nn.Parameter(torch.zeros(nf))
 
     def forward(self, x):
-        size_out = x.size()[:-1] + (self.nf,)
-        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
+        size_out =  (-1, self.n_ctx, self.nf)
+        x = torch.addmm(self.bias, x.view(-1, self.nx), self.weight)
         x = x.view(*size_out)
         return x
 
diff --git a/src/transformers/trainer.py b/src/transformers/trainer.py
index 6524ba4..ade1490 100644
--- a/src/transformers/trainer.py
+++ b/src/transformers/trainer.py
@@ -1,4 +1,5 @@
 import json
+import time
 import logging
 import os
 import random
@@ -22,6 +23,9 @@ from .modeling_utils import PreTrainedModel
 from .optimization import AdamW, get_linear_schedule_with_warmup
 from .training_args import TrainingArguments
 
+from azureml.core.run import Run
+# get the Azure ML run object
+run = Run.get_context()
 
 try:
     from apex import amp
@@ -142,7 +146,7 @@ class Trainer:
         self.eval_dataset = eval_dataset
         self.compute_metrics = compute_metrics
         self.prediction_loss_only = prediction_loss_only
-        if is_tensorboard_available() and self.args.local_rank in [-1, 0]:
+        if is_tensorboard_available() and self.is_world_master():
             self.tb_writer = SummaryWriter(log_dir=self.args.logging_dir)
         if not is_tensorboard_available():
             logger.warning(
@@ -150,7 +154,7 @@ class Trainer:
             )
         set_seed(self.args.seed)
         # Create output directory if needed
-        if self.args.local_rank in [-1, 0]:
+        if self.is_world_master():
             os.makedirs(self.args.output_dir, exist_ok=True)
 
     def get_train_dataloader(self) -> DataLoader:
@@ -298,6 +302,7 @@ class Trainer:
 
         tr_loss = 0.0
         logging_loss = 0.0
+        global_batch_train_start = time.time()
         model.zero_grad()
         train_iterator = trange(
             epochs_trained, int(num_train_epochs), desc="Epoch", disable=self.args.local_rank not in [-1, 0],
@@ -327,6 +332,8 @@ class Trainer:
                     scheduler.step()
                     model.zero_grad()
                     global_step += 1
+                    global_batch_train_duration = time.time() - global_batch_train_start
+                    global_batch_train_start = time.time()
 
                     if self.args.local_rank in [-1, 0]:
                         if (self.args.logging_steps > 0 and global_step % self.args.logging_steps == 0) or (
@@ -343,11 +350,14 @@ class Trainer:
                             learning_rate_scalar = scheduler.get_last_lr()[0]
                             logs["learning_rate"] = learning_rate_scalar
                             logs["loss"] = loss_scalar
+                            logs["global_step"] = global_step
+                            logs["global_step_time"] = global_batch_train_duration
                             logging_loss = tr_loss
 
                             if self.tb_writer:
                                 for k, v in logs.items():
                                     self.tb_writer.add_scalar(k, v, global_step)
+                                    run.log(k,v)
                             epoch_iterator.write(json.dumps({**logs, **{"step": global_step}}))
 
                         if self.args.save_steps > 0 and global_step % self.args.save_steps == 0:
@@ -361,9 +371,10 @@ class Trainer:
                             output_dir = os.path.join(self.args.output_dir, f"{PREFIX_CHECKPOINT_DIR}-{global_step}")
                             self.save_model(output_dir)
                             self._rotate_checkpoints()
-                            torch.save(optimizer.state_dict(), os.path.join(output_dir, "optimizer.pt"))
-                            torch.save(scheduler.state_dict(), os.path.join(output_dir, "scheduler.pt"))
-                            logger.info("Saving optimizer and scheduler states to %s", output_dir)
+                            if self.is_world_master():
+                                torch.save(optimizer.state_dict(), os.path.join(output_dir, "optimizer.pt"))
+                                torch.save(scheduler.state_dict(), os.path.join(output_dir, "scheduler.pt"))
+                                logger.info("Saving optimizer and scheduler states to %s", output_dir)
 
                 if self.args.max_steps > 0 and global_step > self.args.max_steps:
                     epoch_iterator.close()
@@ -553,4 +564,4 @@ class Trainer:
         if len(eval_losses) > 0:
             metrics["loss"] = np.mean(eval_losses)
 
-        return PredictionOutput(predictions=preds, label_ids=label_ids, metrics=metrics)
+        return PredictionOutput(predictions=preds, label_ids=label_ids, metrics=metrics)
\ No newline at end of file
diff --git a/src/transformers/training_args.py b/src/transformers/training_args.py
index af32eac..7f85da9 100644
--- a/src/transformers/training_args.py
+++ b/src/transformers/training_args.py
@@ -1,11 +1,12 @@
 import dataclasses
 import json
 import logging
+import os
 from dataclasses import dataclass, field
 from typing import Optional, Tuple
 
 from .file_utils import cached_property, is_torch_available, torch_required
-
+from .azureml_adapter import set_environment_variables_for_nccl_backend, get_local_rank, get_world_rank, get_global_size, get_local_size
 
 if is_torch_available():
     import torch
@@ -82,6 +83,11 @@ class TrainingArguments:
         },
     )
     local_rank: int = field(default=-1, metadata={"help": "For distributed training: local_rank"})
+    world_rank: int = field(default=-1, metadata={"help": "For distributed training: world_rank"})
+    world_size: int = field(default=1, metadata={"help": "For distributed training: world_size"})
+    master_port: int = field(default=12345, metadata={"help": "For distributed training: free port on rank 0 node"})
+    master_node: str = field(default="localhost", metadata={"help": "For distributed training: address of rank 0 node"})
+    ort_trainer: bool = field(default=False, metadata={"help": "Use ORT to train instead of PyTorch"})
 
     @property
     def train_batch_size(self) -> int:
@@ -126,3 +132,33 @@ class TrainingArguments:
         Serializes this instance to a JSON string.
         """
         return json.dumps(dataclasses.asdict(self), indent=2)
+
+    def update_args(self):
+        from mpi4py import MPI
+        comm = MPI.COMM_WORLD
+
+        has_aml = 'AZ_BATCH_MASTER_NODE' in os.environ.keys() or 'AZ_BATCHAI_MPI_MASTER_NODE' in os.environ.keys()
+        if not has_aml:
+            print('Detected local run')
+            self.world_size = comm.Get_size()
+            self.local_rank = comm.Get_rank() % torch.cuda.device_count()
+            self.world_rank = comm.Get_rank()
+
+
+            os.environ['RANK'] = str(self.world_rank)
+            os.environ['WORLD_SIZE'] = str(self.world_size)
+            os.environ['MASTER_ADDR'] = self.master_node
+            os.environ['MASTER_PORT'] = str(self.master_port)
+
+        else:
+            print('Detected Azure batch run')
+            set_environment_variables_for_nccl_backend(get_local_size() == get_global_size())
+            self.local_rank = get_local_rank()
+            self.world_rank = get_world_rank()
+            self.world_size = get_global_size()
+
+            print('Local size: {}'.format(get_local_size()))
+        print('Local rank: {}'.format(self.local_rank))
+        print('World rank: {}'.format(self.world_rank))
+        print('World size: {}'.format(self.world_size))
+        print('CUDA device: {}'.format(self.local_rank))
